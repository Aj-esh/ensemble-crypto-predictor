{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2982447e",
      "metadata": {
        "id": "2982447e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yfinance as yf\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa1103fa",
      "metadata": {
        "id": "fa1103fa"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float()\n",
        "                    * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FixedEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(FixedEmbedding, self).__init__()\n",
        "\n",
        "        w = torch.zeros(c_in, d_model).float()\n",
        "        w.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float()\n",
        "                    * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        w[:, 0::2] = torch.sin(position * div_term)\n",
        "        w[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.emb = nn.Embedding(c_in, d_model)\n",
        "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x).detach()\n",
        "\n",
        "\n",
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        minute_size = 4\n",
        "        hour_size = 24\n",
        "        weekday_size = 7\n",
        "        day_size = 32\n",
        "        month_size = 13\n",
        "\n",
        "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
        "        if freq == 't':\n",
        "            self.minute_embed = Embed(minute_size, d_model)\n",
        "        self.hour_embed = Embed(hour_size, d_model)\n",
        "        self.weekday_embed = Embed(weekday_size, d_model)\n",
        "        self.day_embed = Embed(day_size, d_model)\n",
        "        self.month_embed = Embed(month_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
        "            self, 'minute_embed') else 0.\n",
        "        hour_x = self.hour_embed(x[:, :, 3])\n",
        "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
        "        day_x = self.day_embed(x[:, :, 1])\n",
        "        month_x = self.month_embed(x[:, :, 0])\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
        "\n",
        "\n",
        "class TimeFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
        "        super(TimeFeatureEmbedding, self).__init__()\n",
        "\n",
        "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
        "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
        "        d_inp = freq_map[freq]\n",
        "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        if x_mark is None:\n",
        "            x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        else:\n",
        "            # THE FIX IS HERE: We need to provide x_mark to use temporal_embedding\n",
        "            x = self.value_embedding(\n",
        "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Inception_Block_V1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
        "        super(Inception_Block_V1, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_kernels = num_kernels\n",
        "        kernels = []\n",
        "        for i in range(self.num_kernels):\n",
        "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))\n",
        "        self.kernels = nn.ModuleList(kernels)\n",
        "        if init_weight:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res_list = []\n",
        "        for i in range(self.num_kernels):\n",
        "            res_list.append(self.kernels[i](x))\n",
        "        res = torch.cat(res_list, dim=1)\n",
        "        return res\n",
        "\n",
        "def FFT_for_Period(x, k=2):\n",
        "    # [B, T, C]\n",
        "    xf = torch.fft.rfft(x, dim=1)\n",
        "    # find period by amplitudes\n",
        "    frequency_list = abs(xf).mean(0).mean(-1)\n",
        "    frequency_list[0] = 0\n",
        "    _, top_list = torch.topk(frequency_list, k)\n",
        "    top_list = top_list.detach().cpu().numpy()\n",
        "    period = x.shape[1] // top_list\n",
        "    return period, abs(xf).mean(-1)[:, top_list]\n",
        "\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(TimesBlock, self).__init__()\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.pred_len = configs.pred_len\n",
        "        self.k = configs.top_k\n",
        "        # parameter-efficient design\n",
        "        self.conv = nn.Sequential(\n",
        "            Inception_Block_V1(configs.d_model, configs.d_ff,\n",
        "                               num_kernels=configs.num_kernels),\n",
        "            nn.GELU(),\n",
        "            Inception_Block_V1(configs.d_ff * configs.num_kernels, configs.d_model,\n",
        "                               num_kernels=configs.num_kernels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N = x.size()\n",
        "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
        "\n",
        "        res = []\n",
        "        for i in range(self.k):\n",
        "            period = period_list[i]\n",
        "            # padding\n",
        "            if (self.seq_len + self.pred_len) % period != 0:\n",
        "                length = (\n",
        "                                 ((self.seq_len + self.pred_len) // period) + 1) * period\n",
        "                padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
        "                out = torch.cat([x, padding], dim=1)\n",
        "            else:\n",
        "                length = (self.seq_len + self.pred_len)\n",
        "                out = x\n",
        "            # reshape\n",
        "            out = out.reshape(B, length // period, period,\n",
        "                              N).permute(0, 3, 1, 2).contiguous()\n",
        "            # 2D conv: from 1d Variation to 2d Variation\n",
        "            out = self.conv(out)\n",
        "            # reshape back\n",
        "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
        "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
        "        res = torch.stack(res, dim=-1)\n",
        "        # adaptive aggregation\n",
        "        period_weight = F.softmax(period_weight, dim=1)\n",
        "        period_weight = period_weight.unsqueeze(\n",
        "            1).unsqueeze(1).repeat(1, T, N, 1)\n",
        "        res = torch.sum(res * period_weight, -1)\n",
        "        # residual connection\n",
        "        res = res + x\n",
        "        return res\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Paper link: https://openreview.net/pdf?id=ju_Uqw384Oq\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, configs):\n",
        "        super(Model, self).__init__()\n",
        "        self.configs = configs\n",
        "        self.task_name = configs.task_name\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.label_len = configs.label_len\n",
        "        self.pred_len = configs.pred_len\n",
        "        self.model = nn.ModuleList([TimesBlock(configs)\n",
        "                                    for _ in range(configs.e_layers)])\n",
        "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                           configs.dropout)\n",
        "        self.layer = configs.e_layers\n",
        "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
        "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
        "            self.predict_linear = nn.Linear(\n",
        "                self.seq_len, self.pred_len + self.seq_len)\n",
        "            self.projection = nn.Linear(\n",
        "                configs.d_model, configs.c_out, bias=True)\n",
        "        # ... (rest of the Model class)\n",
        "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
        "        # Normalization from Non-stationary Transformer\n",
        "        means = x_enc.mean(1, keepdim=True).detach()\n",
        "        x_enc = x_enc.sub(means)\n",
        "        stdev = torch.sqrt(\n",
        "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
        "        x_enc = x_enc.div(stdev)\n",
        "\n",
        "        # embedding\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C]\n",
        "        enc_out = self.predict_linear(enc_out.permute(0, 2, 1)).permute(\n",
        "            0, 2, 1)  # align temporal dimension\n",
        "        # TimesNet\n",
        "        for i in range(self.layer):\n",
        "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
        "        # project back\n",
        "        dec_out = self.projection(enc_out)\n",
        "\n",
        "        # De-Normalization from Non-stationary Transformer\n",
        "        dec_out = dec_out.mul(stdev)\n",
        "        dec_out = dec_out.add(means)\n",
        "        return dec_out\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
        "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
        "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
        "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
        "        # ... (rest of the forward method)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5ad28a7f",
      "metadata": {
        "id": "5ad28a7f"
      },
      "outputs": [],
      "source": [
        "def create_time_features(df, time_col='Date', freq='d'):\n",
        "    \"\"\"\n",
        "    Creates time series features from a datetime index.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    df['month'] = df[time_col].dt.month\n",
        "    df['day'] = df[time_col].dt.day\n",
        "    df['weekday'] = df[time_col].dt.dayofweek\n",
        "    # Add more features if needed, like 'weekofyear'\n",
        "\n",
        "    # The TimeFeatureEmbedding expects these features in a specific order based on `freq`\n",
        "    if freq == 'd': # Daily\n",
        "        feature_cols = ['month', 'day', 'weekday']\n",
        "    else: # Add more logic for other frequencies if needed\n",
        "        raise NotImplementedError(f\"Frequency '{freq}' not implemented.\")\n",
        "\n",
        "    return df[feature_cols].values\n",
        "\n",
        "# This function creates input/output sequences\n",
        "def create_sequences(input_data, time_features, seq_len, pred_len):\n",
        "    X, y, X_mark = [], [], []\n",
        "    for i in range(len(input_data) - seq_len - pred_len + 1):\n",
        "        X.append(input_data[i:(i + seq_len)])\n",
        "        y.append(input_data[i + seq_len:i + seq_len + pred_len])\n",
        "        X_mark.append(time_features[i:(i + seq_len)])\n",
        "    return np.array(X), np.array(y), np.array(X_mark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e2b8bc60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2b8bc60",
        "outputId": "20679484-fc1e-4719-8e12-1cb46a0f45e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2504293634.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  btc_data = yf.download(ticker, start='2014-01-01', end='2024-01-01')\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "ticker = 'BTC-USD'\n",
        "btc_data = yf.download(ticker, start='2014-01-01', end='2024-01-01')\n",
        "btc_data.reset_index(inplace=True) # Move 'Date' from index to a column\n",
        "\n",
        "# Feature Engineering\n",
        "# 'Close' price - forecasting\n",
        "price_data = btc_data[['Close']].values\n",
        "# Create time features\n",
        "time_features = create_time_features(btc_data, time_col='Date', freq='d')\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "# Fit only on the training data to avoid data leakage\n",
        "train_size = int(len(price_data) * 0.8)\n",
        "scaler.fit(price_data[:train_size])\n",
        "scaled_data = scaler.transform(price_data)\n",
        "\n",
        "# Create Sequences\n",
        "configs = type(\"Configs\", (), {})() # Create a mock configs class\n",
        "configs.seq_len = 30 # Use 30 days of history\n",
        "configs.pred_len = 1 # Predict 1 day ahead\n",
        "configs.enc_in = 1 # We are using 1 feature (Close price)\n",
        "configs.c_out = 1\n",
        "\n",
        "X, y, X_mark = create_sequences(scaled_data, time_features, configs.seq_len, configs.pred_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b5faa2a3",
      "metadata": {
        "id": "b5faa2a3"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "X_train, X_test, y_train, y_test, X_mark_train, X_mark_test = train_test_split(\n",
        "    X, y, X_mark, test_size=0.2, shuffle=False # Time series data should not be shuffled\n",
        ")\n",
        "\n",
        "# to Tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_mark_train = torch.tensor(X_mark_train, dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "X_mark_test = torch.tensor(X_mark_test, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f4ZM-Va4vWm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4ZM-Va4vWm",
        "outputId": "69cc68d5-8b90-46f4-ee70-eb66781297f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2690, 30, 1])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f866b743",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f866b743",
        "outputId": "19d14f5b-88d7-431d-f7ca-1e3432ee65b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#  3. Model Training\n",
        "# Hyperparameters for the Model\n",
        "configs.task_name = 'long_term_forecast'\n",
        "configs.label_len = 0\n",
        "configs.e_layers = 1\n",
        "configs.d_model = 32\n",
        "configs.d_ff = 64\n",
        "configs.num_kernels = 3\n",
        "configs.embed = 'timeF' # Use TimeFeatureEmbedding\n",
        "configs.freq = 'd'\n",
        "configs.dropout = 0.05\n",
        "configs.top_k = 3\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize Model and Optimizer\n",
        "model = Model(configs).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # Slightly increased learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4514dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af4514dd",
        "outputId": "4c370a58-5cc3-4171-a223-f458cb22b194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Loss: 0.054949\n",
            "Saved best model with loss: 0.054949\n",
            "Epoch [2/200], Loss: 0.173463\n",
            "Epoch [3/200], Loss: 0.079957\n",
            "Epoch [4/200], Loss: 0.032963\n",
            "Saved best model with loss: 0.032963\n",
            "Epoch [5/200], Loss: 0.023362\n",
            "Saved best model with loss: 0.023362\n",
            "Epoch [6/200], Loss: 0.026210\n",
            "Epoch [7/200], Loss: 0.028789\n",
            "Epoch [8/200], Loss: 0.028183\n",
            "Epoch [9/200], Loss: 0.026223\n",
            "Epoch [10/200], Loss: 0.024501\n",
            "Epoch [11/200], Loss: 0.023616\n",
            "Epoch [12/200], Loss: 0.023360\n",
            "Saved best model with loss: 0.023360\n",
            "Epoch [13/200], Loss: 0.023509\n",
            "Epoch [14/200], Loss: 0.023763\n",
            "Epoch [15/200], Loss: 0.023959\n",
            "Epoch [16/200], Loss: 0.023923\n",
            "Epoch [17/200], Loss: 0.023667\n",
            "Epoch [18/200], Loss: 0.023217\n",
            "Saved best model with loss: 0.023217\n",
            "Epoch [19/200], Loss: 0.022776\n",
            "Saved best model with loss: 0.022776\n",
            "Epoch [20/200], Loss: 0.022175\n",
            "Saved best model with loss: 0.022175\n",
            "Epoch [21/200], Loss: 0.021917\n",
            "Saved best model with loss: 0.021917\n",
            "Epoch [22/200], Loss: 0.021817\n",
            "Saved best model with loss: 0.021817\n",
            "Epoch [23/200], Loss: 0.021750\n",
            "Saved best model with loss: 0.021750\n",
            "Epoch [24/200], Loss: 0.021868\n",
            "Epoch [25/200], Loss: 0.021779\n",
            "Epoch [26/200], Loss: 0.021343\n",
            "Saved best model with loss: 0.021343\n",
            "Epoch [27/200], Loss: 0.020846\n",
            "Saved best model with loss: 0.020846\n",
            "Epoch [28/200], Loss: 0.020295\n",
            "Saved best model with loss: 0.020295\n",
            "Epoch [29/200], Loss: 0.019899\n",
            "Saved best model with loss: 0.019899\n",
            "Epoch [30/200], Loss: 0.019689\n",
            "Saved best model with loss: 0.019689\n",
            "Epoch [31/200], Loss: 0.019604\n",
            "Saved best model with loss: 0.019604\n",
            "Epoch [32/200], Loss: 0.019073\n",
            "Saved best model with loss: 0.019073\n",
            "Epoch [33/200], Loss: 0.018534\n",
            "Saved best model with loss: 0.018534\n",
            "Epoch [34/200], Loss: 0.017897\n",
            "Saved best model with loss: 0.017897\n",
            "Epoch [35/200], Loss: 0.016921\n",
            "Saved best model with loss: 0.016921\n",
            "Epoch [36/200], Loss: 0.016413\n",
            "Saved best model with loss: 0.016413\n",
            "Epoch [37/200], Loss: 0.015584\n",
            "Saved best model with loss: 0.015584\n",
            "Epoch [38/200], Loss: 0.014886\n",
            "Saved best model with loss: 0.014886\n",
            "Epoch [39/200], Loss: 0.013770\n",
            "Saved best model with loss: 0.013770\n",
            "Epoch [40/200], Loss: 0.012673\n",
            "Saved best model with loss: 0.012673\n",
            "Epoch [41/200], Loss: 0.012107\n",
            "Saved best model with loss: 0.012107\n",
            "Epoch [42/200], Loss: 0.011101\n",
            "Saved best model with loss: 0.011101\n",
            "Epoch [43/200], Loss: 0.010569\n",
            "Saved best model with loss: 0.010569\n",
            "Epoch [44/200], Loss: 0.010000\n",
            "Saved best model with loss: 0.010000\n",
            "Epoch [45/200], Loss: 0.009692\n",
            "Saved best model with loss: 0.009692\n",
            "Epoch [46/200], Loss: 0.009142\n",
            "Saved best model with loss: 0.009142\n",
            "Epoch [47/200], Loss: 0.008622\n",
            "Saved best model with loss: 0.008622\n",
            "Epoch [48/200], Loss: 0.008007\n",
            "Saved best model with loss: 0.008007\n",
            "Epoch [49/200], Loss: 0.007738\n",
            "Saved best model with loss: 0.007738\n",
            "Epoch [50/200], Loss: 0.007430\n",
            "Saved best model with loss: 0.007430\n",
            "Epoch [51/200], Loss: 0.007273\n",
            "Saved best model with loss: 0.007273\n",
            "Epoch [52/200], Loss: 0.007156\n",
            "Saved best model with loss: 0.007156\n",
            "Epoch [53/200], Loss: 0.007025\n",
            "Saved best model with loss: 0.007025\n",
            "Epoch [54/200], Loss: 0.006933\n",
            "Saved best model with loss: 0.006933\n",
            "Epoch [55/200], Loss: 0.006562\n",
            "Saved best model with loss: 0.006562\n",
            "Epoch [56/200], Loss: 0.006152\n",
            "Saved best model with loss: 0.006152\n",
            "Epoch [57/200], Loss: 0.006081\n",
            "Saved best model with loss: 0.006081\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "epochs = 200\n",
        "epoch = 0\n",
        "batch_size = 2690\n",
        "best_loss = float('inf')\n",
        "best_model_path = f'models/best_timesnet_model_{epoch}.pth'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        # Get batch and move to device\n",
        "        batch_X = X_train[i:i+batch_size].to(device)\n",
        "        batch_y = y_train[i:i+batch_size].to(device)\n",
        "        batch_X_mark = X_mark_train[i:i+batch_size].to(device)\n",
        "\n",
        "        # Zeros for decoder inputs, not used in this forecasting model\n",
        "        batch_dec_input = torch.zeros_like(batch_y).to(device) # Placeholder\n",
        "        batch_dec_mark = torch.zeros_like(batch_y).to(device) # Placeholder\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch_X, batch_X_mark, batch_dec_input, batch_dec_mark)\n",
        "\n",
        "        # Output shape: [batch_size, pred_len, features]\n",
        "        # Target shape: [batch_size, pred_len, features]\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / (len(X_train) / batch_size)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_epoch_loss:.6f}')\n",
        "\n",
        "    # Save the best model\n",
        "    if avg_epoch_loss < best_loss:\n",
        "        best_loss = avg_epoch_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f'Saved best model with loss: {best_loss:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0890bb52",
      "metadata": {
        "id": "0890bb52"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval() # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e306d9",
      "metadata": {
        "id": "f0e306d9"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    test_loss = 0\n",
        "    # Use the full test dataset for evaluation\n",
        "    batch_X_test = X_test.to(device)\n",
        "    batch_y_test = y_test.to(device)\n",
        "    batch_X_mark_test = X_mark_test.to(device)\n",
        "\n",
        "    # Zeros for decoder inputs, not used in this forecasting model\n",
        "    batch_dec_input_test = torch.zeros_like(batch_y_test).to(device) # Placeholder\n",
        "    batch_dec_mark_test = torch.zeros_like(batch_y_test).to(device) # Placeholder\n",
        "\n",
        "    outputs_test = model(batch_X_test, batch_X_mark_test, batch_dec_input_test, batch_dec_mark_test)\n",
        "\n",
        "    loss_test = criterion(outputs_test, batch_y_test)\n",
        "    test_loss = loss_test.item()\n",
        "\n",
        "print(f'Test Loss: {test_loss:.6f}')\n",
        "\n",
        "# Optional: Inverse transform the predictions and actual values for plotting or further analysis\n",
        "predicted = outputs_test.cpu().numpy()\n",
        "actual = batch_y_test.cpu().numpy()\n",
        "\n",
        "# Reshape for inverse transform: (n_samples * pred_len, n_features)\n",
        "predicted_reshaped = predicted.reshape(-1, predicted.shape[-1])\n",
        "actual_reshaped = actual.reshape(-1, actual.shape[-1])\n",
        "\n",
        "predicted_original_scale = scaler.inverse_transform(predicted_reshaped)\n",
        "actual_original_scale = scaler.inverse_transform(actual_reshaped)\n",
        "\n",
        "# Reshape back to original prediction shape: (n_samples, pred_len, n_features)\n",
        "predicted_original_scale = predicted_original_scale.reshape(predicted.shape)\n",
        "actual_original_scale = actual_original_scale.reshape(actual.shape)\n",
        "\n",
        "print(\"Predictions in original scale (first 5):\")\n",
        "print(predicted_original_scale[:5])\n",
        "print(\"\\nActual values in original scale (first 5):\")\n",
        "print(actual_original_scale[:5])\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Since pred_len is 1, we can plot the predicted vs actual points directly\n",
        "# For pred_len > 1, you might need to adjust how you visualize the sequences\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(actual_original_scale.flatten(), label='Actual')\n",
        "plt.plot(predicted_original_scale.flatten(), label='Predicted')\n",
        "plt.title('Bitcoin Price Prediction (Test Set)')\n",
        "plt.xlabel('Time (days)')\n",
        "plt.ylabel('Price (USD)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f1cecc9",
      "metadata": {
        "id": "9f1cecc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
